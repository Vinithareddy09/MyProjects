# Step 1: Importing necessary libraries
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
# Load the dataset with the correct separator (semicolon)
df = pd.read_csv('macroeconomics.csv', sep=';')  # Specify the separator as semicolon

# Explore the dataset: first few rows
print(df.head())

# Check the dataset info (data types, non-null counts)
print(df.info())

# Summary statistics for numerical features
print(df.describe())
# Check for non-numeric columns
print(df.dtypes)

# Select only numerical columns and fill missing values
df_numerical = df.select_dtypes(include=['float64', 'int64'])
df_numerical.fillna(df_numerical.mean(), inplace=True)

# Optionally, if you have date columns, you can convert them:
# df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

# Print the first few rows after filling missing values
print(df.head())
print(df.columns)
# Step 4: Encode categorical variables (if any)
# Identified categorical columns: ['date', 'account_balance']
df = pd.get_dummies(df, columns=['date', 'account_balance'], drop_first=True)
# Define the target variable 'y' using 'core_inflation'
y = df['core_inflation']

# Ensure the target column 'core_inflation' is present and separate it from features
if 'core_inflation' in df.columns:
    X = df.drop(columns=['core_inflation'])  # Features
    y = df['core_inflation']  # Target variable
else:
    raise ValueError("'core_inflation' column not found in the dataset.")

# Initialize the StandardScaler
scaler = StandardScaler()

# Apply standardization (z-score normalization)
X_scaled = scaler.fit_transform(X)
# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
# Initialize and train the model
model = LinearRegression()
model.fit(X_train, y_train)
# Step 8: Evaluate the Model

# Predict the target variable on the test set
y_pred = model.predict(X_test)

# Calculate evaluation metrics
mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error
mse = mean_squared_error(y_test, y_pred)   # Mean Squared Error
rmse = np.sqrt(mse)  # Root Mean Squared Error
r2 = r2_score(y_test, y_pred)  # R-squared score

# Display the evaluation metrics
print("\nModel Evaluation Metrics:")
print(f'Mean Absolute Error (MAE): {mae}')
print(f'Mean Squared Error (MSE): {mse}')
print(f'Root Mean Squared Error (RMSE): {rmse}')
print(f'R-squared (RÂ²): {r2}')
print(f'Predicted Value: {y_pred}')# Step 9: Plot Actual vs. Predicted Values

import matplotlib.pyplot as plt

# Plot actual vs. predicted values
plt.figure(figsize=(10, 6))
plt.plot(y_test.values, label='Actual Values', marker='o')
plt.plot(y_pred, label='Predicted Values', marker='x')
plt.title('Actual vs Predicted Values of Core Inflation')
plt.xlabel('Sample Index')
plt.ylabel('Core Inflation')
plt.legend()
plt.show()